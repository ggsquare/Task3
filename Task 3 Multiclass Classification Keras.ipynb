{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tables import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([45324, 45325, 45326, 45327, 45328, 45329, 45330, 45331, 45332,\n",
       "            45333,\n",
       "            ...\n",
       "            53451, 53452, 53453, 53454, 53455, 53456, 53457, 53458, 53459,\n",
       "            53460],\n",
       "           dtype='int64', length=8137)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_hdf(\"train.h5\", \"train\")\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "#View training data\n",
    "#train.shape (45324, 101)\n",
    "test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['y']\n",
    "x_train = train._drop_axis(['y'], axis=1)\n",
    "\n",
    "#Switch to numpy\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00011446602163711628"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len(x_train[1])\n",
    "total = 0\n",
    "\n",
    "#standardize data, so for each x1..x100, do (x-mean)/sd(x) s.t. mean(x) ~= 0\n",
    "for i in range(num_features):\n",
    "    x_train[:,i] = (x_train[:,i] - np.mean(x_train[:,i])) / np.std(x_train[:,i])\n",
    "    x_test[:,i] = (x_test[:,i] - np.mean(x_test[:,i])) / np.std(x_test[:,i])\n",
    "    total += np.mean(x_train[:,i])\n",
    "\n",
    "\n",
    "#adding all the means, we still have a very small # close to 0\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#Define model\n",
    "model = Sequential([Dense(70, input_shape=(100,)), #a bit counterintuitve\n",
    "                   Activation('relu'),\n",
    "                   Dense(30),\n",
    "                   Activation('relu'),\n",
    "                   Dense(50),\n",
    "                   Activation('relu'),\n",
    "                   Dense(20),\n",
    "                   Activation('relu'),\n",
    "                   Dense(5),\n",
    "                   Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.4981 - acc: 0.8135\n",
      "Epoch 2/75\n",
      "45324/45324 [==============================] - 1s 32us/step - loss: 0.3223 - acc: 0.8850\n",
      "Epoch 3/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.2851 - acc: 0.8996\n",
      "Epoch 4/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.2633 - acc: 0.9070\n",
      "Epoch 5/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.2453 - acc: 0.9138\n",
      "Epoch 6/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.2355 - acc: 0.9170\n",
      "Epoch 7/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.2252 - acc: 0.9204\n",
      "Epoch 8/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.2170 - acc: 0.9228\n",
      "Epoch 9/75\n",
      "45324/45324 [==============================] - 2s 33us/step - loss: 0.2075 - acc: 0.9272\n",
      "Epoch 10/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.2045 - acc: 0.9276\n",
      "Epoch 11/75\n",
      "45324/45324 [==============================] - 2s 37us/step - loss: 0.1979 - acc: 0.9303\n",
      "Epoch 12/75\n",
      "45324/45324 [==============================] - 2s 36us/step - loss: 0.1923 - acc: 0.9318\n",
      "Epoch 13/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.1864 - acc: 0.9347\n",
      "Epoch 14/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.1837 - acc: 0.9346\n",
      "Epoch 15/75\n",
      "45324/45324 [==============================] - 2s 39us/step - loss: 0.1807 - acc: 0.9355\n",
      "Epoch 16/75\n",
      "45324/45324 [==============================] - 2s 37us/step - loss: 0.1757 - acc: 0.9378\n",
      "Epoch 17/75\n",
      "45324/45324 [==============================] - 2s 40us/step - loss: 0.1719 - acc: 0.9380\n",
      "Epoch 18/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1678 - acc: 0.9403\n",
      "Epoch 19/75\n",
      "45324/45324 [==============================] - 1s 28us/step - loss: 0.1648 - acc: 0.9405\n",
      "Epoch 20/75\n",
      "45324/45324 [==============================] - 1s 28us/step - loss: 0.1629 - acc: 0.9415\n",
      "Epoch 21/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1577 - acc: 0.9432\n",
      "Epoch 22/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.1576 - acc: 0.9434\n",
      "Epoch 23/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.1533 - acc: 0.9454\n",
      "Epoch 24/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.1517 - acc: 0.9455\n",
      "Epoch 25/75\n",
      "45324/45324 [==============================] - 1s 31us/step - loss: 0.1474 - acc: 0.9472\n",
      "Epoch 26/75\n",
      "45324/45324 [==============================] - 2s 40us/step - loss: 0.1472 - acc: 0.9455\n",
      "Epoch 27/75\n",
      "45324/45324 [==============================] - 2s 38us/step - loss: 0.1433 - acc: 0.9479\n",
      "Epoch 28/75\n",
      "45324/45324 [==============================] - 2s 33us/step - loss: 0.1424 - acc: 0.9479\n",
      "Epoch 29/75\n",
      "45324/45324 [==============================] - 2s 41us/step - loss: 0.1399 - acc: 0.9497\n",
      "Epoch 30/75\n",
      "45324/45324 [==============================] - 1s 31us/step - loss: 0.1375 - acc: 0.9497\n",
      "Epoch 31/75\n",
      "45324/45324 [==============================] - 1s 31us/step - loss: 0.1363 - acc: 0.9512\n",
      "Epoch 32/75\n",
      "45324/45324 [==============================] - 2s 33us/step - loss: 0.1337 - acc: 0.9510\n",
      "Epoch 33/75\n",
      "45324/45324 [==============================] - 2s 35us/step - loss: 0.1325 - acc: 0.9528\n",
      "Epoch 34/75\n",
      "45324/45324 [==============================] - 2s 38us/step - loss: 0.1292 - acc: 0.9537\n",
      "Epoch 35/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.1282 - acc: 0.9533\n",
      "Epoch 36/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.1276 - acc: 0.9530\n",
      "Epoch 37/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.1246 - acc: 0.9550\n",
      "Epoch 38/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.1231 - acc: 0.9554\n",
      "Epoch 39/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.1221 - acc: 0.9561\n",
      "Epoch 40/75\n",
      "45324/45324 [==============================] - 2s 40us/step - loss: 0.1201 - acc: 0.9568\n",
      "Epoch 41/75\n",
      "45324/45324 [==============================] - 2s 35us/step - loss: 0.1193 - acc: 0.9569\n",
      "Epoch 42/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1180 - acc: 0.9569\n",
      "Epoch 43/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.1167 - acc: 0.9577\n",
      "Epoch 44/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1154 - acc: 0.9583\n",
      "Epoch 45/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1137 - acc: 0.9586\n",
      "Epoch 46/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1132 - acc: 0.9594\n",
      "Epoch 47/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.1108 - acc: 0.9599\n",
      "Epoch 48/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1098 - acc: 0.9593\n",
      "Epoch 49/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.1093 - acc: 0.9603\n",
      "Epoch 50/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.1074 - acc: 0.9611\n",
      "Epoch 51/75\n",
      "45324/45324 [==============================] - 1s 32us/step - loss: 0.1093 - acc: 0.9609\n",
      "Epoch 52/75\n",
      "45324/45324 [==============================] - 1s 32us/step - loss: 0.1062 - acc: 0.9611\n",
      "Epoch 53/75\n",
      "45324/45324 [==============================] - 1s 32us/step - loss: 0.1021 - acc: 0.9629\n",
      "Epoch 54/75\n",
      "45324/45324 [==============================] - 1s 31us/step - loss: 0.1047 - acc: 0.9620\n",
      "Epoch 55/75\n",
      "45324/45324 [==============================] - 1s 32us/step - loss: 0.1038 - acc: 0.9631\n",
      "Epoch 56/75\n",
      "45324/45324 [==============================] - 1s 28us/step - loss: 0.1018 - acc: 0.9633\n",
      "Epoch 57/75\n",
      "45324/45324 [==============================] - 1s 31us/step - loss: 0.0998 - acc: 0.9630\n",
      "Epoch 58/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.0991 - acc: 0.9643\n",
      "Epoch 59/75\n",
      "45324/45324 [==============================] - 2s 35us/step - loss: 0.0990 - acc: 0.9639\n",
      "Epoch 60/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.0963 - acc: 0.9645\n",
      "Epoch 61/75\n",
      "45324/45324 [==============================] - 1s 28us/step - loss: 0.0979 - acc: 0.9637\n",
      "Epoch 62/75\n",
      "45324/45324 [==============================] - 1s 27us/step - loss: 0.0947 - acc: 0.9664\n",
      "Epoch 63/75\n",
      "45324/45324 [==============================] - 1s 28us/step - loss: 0.0962 - acc: 0.9649\n",
      "Epoch 64/75\n",
      "45324/45324 [==============================] - 1s 28us/step - loss: 0.0936 - acc: 0.9663\n",
      "Epoch 65/75\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.0932 - acc: 0.9663\n",
      "Epoch 66/75\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.0926 - acc: 0.9659\n",
      "Epoch 67/75\n",
      "45324/45324 [==============================] - 2s 38us/step - loss: 0.0930 - acc: 0.9665\n",
      "Epoch 68/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.0891 - acc: 0.9677\n",
      "Epoch 69/75\n",
      "45324/45324 [==============================] - 2s 33us/step - loss: 0.0907 - acc: 0.9665\n",
      "Epoch 70/75\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.0904 - acc: 0.9668\n",
      "Epoch 71/75\n",
      "45324/45324 [==============================] - 2s 33us/step - loss: 0.0880 - acc: 0.9676\n",
      "Epoch 72/75\n",
      "45324/45324 [==============================] - 2s 37us/step - loss: 0.0884 - acc: 0.9674\n",
      "Epoch 73/75\n",
      "45324/45324 [==============================] - 2s 40us/step - loss: 0.0873 - acc: 0.9684\n",
      "Epoch 74/75\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.0865 - acc: 0.9680\n",
      "Epoch 75/75\n",
      "45324/45324 [==============================] - 2s 36us/step - loss: 0.0863 - acc: 0.9681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b3e3bb6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "#Easy way to convert to one-hot\n",
    "y_train_hot2 = keras.utils.to_categorical(y_train, num_classes=5)\n",
    "\n",
    "model.fit(x_train, y_train_hot2, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_one_hot = model.predict(x_test)\n",
    "type(pred_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert predictions back from 1-hot\n",
    "pred = []\n",
    "\n",
    "for line in pred_one_hot:\n",
    "    pred.append(np.argmax(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Id': test.index, 'y': pred}\n",
    "out = pd.DataFrame(d)\n",
    "out.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = pd.DataFrame([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.to_csv('output_' + '.5' + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foo = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.to_csv('output_' + foo + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
