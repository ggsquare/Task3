{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tables import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>x100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.768117</td>\n",
       "      <td>-0.742052</td>\n",
       "      <td>-0.426554</td>\n",
       "      <td>-0.981865</td>\n",
       "      <td>-0.739627</td>\n",
       "      <td>-0.710880</td>\n",
       "      <td>0.665179</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>-0.621006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.516481</td>\n",
       "      <td>0.809766</td>\n",
       "      <td>-0.272739</td>\n",
       "      <td>0.853082</td>\n",
       "      <td>0.920256</td>\n",
       "      <td>0.477963</td>\n",
       "      <td>-0.001392</td>\n",
       "      <td>0.792493</td>\n",
       "      <td>-0.993887</td>\n",
       "      <td>0.024454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.785173</td>\n",
       "      <td>-0.759261</td>\n",
       "      <td>-0.182494</td>\n",
       "      <td>-0.982210</td>\n",
       "      <td>-0.699956</td>\n",
       "      <td>-0.680122</td>\n",
       "      <td>0.768796</td>\n",
       "      <td>0.833732</td>\n",
       "      <td>-0.677768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.603206</td>\n",
       "      <td>0.818925</td>\n",
       "      <td>-0.168998</td>\n",
       "      <td>0.898659</td>\n",
       "      <td>0.944296</td>\n",
       "      <td>0.296505</td>\n",
       "      <td>0.055101</td>\n",
       "      <td>0.755631</td>\n",
       "      <td>-0.994284</td>\n",
       "      <td>-0.120421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.838828</td>\n",
       "      <td>-0.785715</td>\n",
       "      <td>-0.306292</td>\n",
       "      <td>-0.988036</td>\n",
       "      <td>-0.700962</td>\n",
       "      <td>-0.657455</td>\n",
       "      <td>0.714290</td>\n",
       "      <td>0.879114</td>\n",
       "      <td>-0.706142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.609176</td>\n",
       "      <td>0.855403</td>\n",
       "      <td>-0.372542</td>\n",
       "      <td>0.869567</td>\n",
       "      <td>0.912317</td>\n",
       "      <td>0.348275</td>\n",
       "      <td>-0.044856</td>\n",
       "      <td>0.680716</td>\n",
       "      <td>-0.996272</td>\n",
       "      <td>0.058355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.774104</td>\n",
       "      <td>-0.714600</td>\n",
       "      <td>-0.279582</td>\n",
       "      <td>-0.976956</td>\n",
       "      <td>-0.698350</td>\n",
       "      <td>-0.701044</td>\n",
       "      <td>0.749821</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>-0.641183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502083</td>\n",
       "      <td>0.821006</td>\n",
       "      <td>-0.145995</td>\n",
       "      <td>0.896735</td>\n",
       "      <td>0.950754</td>\n",
       "      <td>0.527539</td>\n",
       "      <td>0.089638</td>\n",
       "      <td>0.782050</td>\n",
       "      <td>-0.994627</td>\n",
       "      <td>-0.112656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.754017</td>\n",
       "      <td>-0.757367</td>\n",
       "      <td>-0.403357</td>\n",
       "      <td>-0.982875</td>\n",
       "      <td>-0.678298</td>\n",
       "      <td>-0.753136</td>\n",
       "      <td>0.704445</td>\n",
       "      <td>0.875914</td>\n",
       "      <td>-0.667183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707570</td>\n",
       "      <td>0.849876</td>\n",
       "      <td>-0.276707</td>\n",
       "      <td>0.863108</td>\n",
       "      <td>0.957781</td>\n",
       "      <td>0.623078</td>\n",
       "      <td>-0.106684</td>\n",
       "      <td>0.702265</td>\n",
       "      <td>-0.995401</td>\n",
       "      <td>0.106640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y        x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  1  0.768117 -0.742052 -0.426554 -0.981865 -0.739627 -0.710880  0.665179   \n",
       "1  1  0.785173 -0.759261 -0.182494 -0.982210 -0.699956 -0.680122  0.768796   \n",
       "2  1  0.838828 -0.785715 -0.306292 -0.988036 -0.700962 -0.657455  0.714290   \n",
       "3  3  0.774104 -0.714600 -0.279582 -0.976956 -0.698350 -0.701044  0.749821   \n",
       "4  1  0.754017 -0.757367 -0.403357 -0.982875 -0.678298 -0.753136  0.704445   \n",
       "\n",
       "         x8        x9    ...          x91       x92       x93       x94  \\\n",
       "0  0.857843 -0.621006    ...    -0.516481  0.809766 -0.272739  0.853082   \n",
       "1  0.833732 -0.677768    ...    -0.603206  0.818925 -0.168998  0.898659   \n",
       "2  0.879114 -0.706142    ...    -0.609176  0.855403 -0.372542  0.869567   \n",
       "3  0.876500 -0.641183    ...    -0.502083  0.821006 -0.145995  0.896735   \n",
       "4  0.875914 -0.667183    ...    -0.707570  0.849876 -0.276707  0.863108   \n",
       "\n",
       "        x95       x96       x97       x98       x99      x100  \n",
       "0  0.920256  0.477963 -0.001392  0.792493 -0.993887  0.024454  \n",
       "1  0.944296  0.296505  0.055101  0.755631 -0.994284 -0.120421  \n",
       "2  0.912317  0.348275 -0.044856  0.680716 -0.996272  0.058355  \n",
       "3  0.950754  0.527539  0.089638  0.782050 -0.994627 -0.112656  \n",
       "4  0.957781  0.623078 -0.106684  0.702265 -0.995401  0.106640  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_hdf(\"train.h5\", \"train\")\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "#train.shape(45324, 101)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['y']\n",
    "x_train = train._drop_axis(['y'], axis=1)\n",
    "\n",
    "#Switch to numpy\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00011446602163711628"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = len(x_train[1])\n",
    "total = 0\n",
    "\n",
    "#standardize data, so for each x1..x100, do (x-mean)/sd(x) s.t. mean(x) ~= 0\n",
    "for i in range(num_features):\n",
    "    x_train[:,i] = (x_train[:,i] - np.mean(x_train[:,i])) / np.std(x_train[:,i])\n",
    "    x_test[:,i] = (x_test[:,i] - np.mean(x_test[:,i])) / np.std(x_test[:,i])\n",
    "    total += np.mean(x_train[:,i])\n",
    "\n",
    "\n",
    "#adding all the means, we still have a very small # close to 0\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model with Keras\n",
    "* Iterate through different regularization parameters (L2 norm)\n",
    "* Output predictions for each lambda into file denoted output_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(70, input_shape=(100,), kernel_regularizer=<keras.reg...)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "45324/45324 [==============================] - 2s 35us/step - loss: 0.6856 - acc: 0.8212\n",
      "Epoch 2/50\n",
      "45324/45324 [==============================] - 2s 38us/step - loss: 0.4286 - acc: 0.8798\n",
      "Epoch 3/50\n",
      "45324/45324 [==============================] - 2s 39us/step - loss: 0.3823 - acc: 0.8903\n",
      "Epoch 4/50\n",
      "45324/45324 [==============================] - 2s 35us/step - loss: 0.3594 - acc: 0.8943\n",
      "Epoch 5/50\n",
      "45324/45324 [==============================] - 2s 34us/step - loss: 0.3472 - acc: 0.8975: 0s - loss: 0.336\n",
      "Epoch 6/50\n",
      "45324/45324 [==============================] - 2s 35us/step - loss: 0.3370 - acc: 0.9020\n",
      "Epoch 7/50\n",
      "45324/45324 [==============================] - 2s 33us/step - loss: 0.3312 - acc: 0.9037\n",
      "Epoch 8/50\n",
      "45324/45324 [==============================] - 1s 33us/step - loss: 0.3268 - acc: 0.9064\n",
      "Epoch 9/50\n",
      "45324/45324 [==============================] - 2s 36us/step - loss: 0.3228 - acc: 0.9078\n",
      "Epoch 10/50\n",
      "45324/45324 [==============================] - 2s 44us/step - loss: 0.3179 - acc: 0.9080\n",
      "Epoch 11/50\n",
      "45324/45324 [==============================] - 2s 39us/step - loss: 0.3168 - acc: 0.9088\n",
      "Epoch 12/50\n",
      "45324/45324 [==============================] - 2s 46us/step - loss: 0.3133 - acc: 0.9111\n",
      "Epoch 13/50\n",
      "45324/45324 [==============================] - 2s 45us/step - loss: 0.3076 - acc: 0.9124\n",
      "Epoch 14/50\n",
      "45324/45324 [==============================] - 2s 40us/step - loss: 0.3095 - acc: 0.9122: 1s - los\n",
      "Epoch 15/50\n",
      "45324/45324 [==============================] - 2s 36us/step - loss: 0.3055 - acc: 0.9125\n",
      "Epoch 16/50\n",
      "45324/45324 [==============================] - 1s 31us/step - loss: 0.3022 - acc: 0.9150\n",
      "Epoch 17/50\n",
      "45324/45324 [==============================] - 1s 29us/step - loss: 0.3027 - acc: 0.9140\n",
      "Epoch 18/50\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.2995 - acc: 0.9161\n",
      "Epoch 19/50\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.2976 - acc: 0.9154\n",
      "Epoch 20/50\n",
      "45324/45324 [==============================] - 1s 30us/step - loss: 0.2978 - acc: 0.9154\n",
      "Epoch 21/50\n",
      "45324/45324 [==============================] - 1s 31us/step - loss: 0.2980 - acc: 0.9157\n",
      "Epoch 22/50\n",
      "45324/45324 [==============================] - 2s 39us/step - loss: 0.2941 - acc: 0.9168\n",
      "Epoch 23/50\n",
      "45324/45324 [==============================] - 2s 37us/step - loss: 0.2948 - acc: 0.9172\n",
      "Epoch 24/50\n",
      "45324/45324 [==============================] - 2s 44us/step - loss: 0.2917 - acc: 0.9175\n",
      "Epoch 25/50\n",
      "45324/45324 [==============================] - 2s 36us/step - loss: 0.2923 - acc: 0.9168\n",
      "Epoch 26/50\n",
      "45324/45324 [==============================] - 2s 40us/step - loss: 0.2901 - acc: 0.9183\n",
      "Epoch 27/50\n",
      "45324/45324 [==============================] - 2s 43us/step - loss: 0.2909 - acc: 0.9173\n",
      "Epoch 28/50\n",
      "45324/45324 [==============================] - 3s 58us/step - loss: 0.2894 - acc: 0.9198\n",
      "Epoch 29/50\n",
      "45324/45324 [==============================] - 2s 47us/step - loss: 0.2887 - acc: 0.9176\n",
      "Epoch 30/50\n",
      "45324/45324 [==============================] - 2s 43us/step - loss: 0.2885 - acc: 0.9189\n",
      "Epoch 31/50\n",
      "45324/45324 [==============================] - 2s 50us/step - loss: 0.2855 - acc: 0.9206\n",
      "Epoch 32/50\n",
      "45324/45324 [==============================] - 2s 49us/step - loss: 0.2840 - acc: 0.9198\n",
      "Epoch 33/50\n",
      "45324/45324 [==============================] - 2s 54us/step - loss: 0.2837 - acc: 0.9213\n",
      "Epoch 34/50\n",
      "45324/45324 [==============================] - 3s 71us/step - loss: 0.2845 - acc: 0.9197\n",
      "Epoch 35/50\n",
      "45324/45324 [==============================] - 2s 52us/step - loss: 0.2823 - acc: 0.9212\n",
      "Epoch 36/50\n",
      "45324/45324 [==============================] - 2s 52us/step - loss: 0.2840 - acc: 0.9213\n",
      "Epoch 37/50\n",
      "45324/45324 [==============================] - 2s 50us/step - loss: 0.2802 - acc: 0.9214\n",
      "Epoch 38/50\n",
      "45324/45324 [==============================] - 2s 50us/step - loss: 0.2837 - acc: 0.9204\n",
      "Epoch 39/50\n",
      "45324/45324 [==============================] - 2s 50us/step - loss: 0.2795 - acc: 0.9214\n",
      "Epoch 40/50\n",
      "45324/45324 [==============================] - 2s 51us/step - loss: 0.2817 - acc: 0.9217\n",
      "Epoch 41/50\n",
      "45324/45324 [==============================] - 2s 51us/step - loss: 0.2806 - acc: 0.9218\n",
      "Epoch 42/50\n",
      "45324/45324 [==============================] - 2s 50us/step - loss: 0.2780 - acc: 0.9225\n",
      "Epoch 43/50\n",
      "45324/45324 [==============================] - 2s 50us/step - loss: 0.2809 - acc: 0.9225\n",
      "Epoch 44/50\n",
      "45324/45324 [==============================] - 2s 50us/step - loss: 0.2770 - acc: 0.9228\n",
      "Epoch 45/50\n",
      "45324/45324 [==============================] - 2s 49us/step - loss: 0.2768 - acc: 0.9232\n",
      "Epoch 46/50\n",
      "45324/45324 [==============================] - 2s 52us/step - loss: 0.2754 - acc: 0.9241\n",
      "Epoch 47/50\n",
      "45324/45324 [==============================] - 3s 66us/step - loss: 0.2774 - acc: 0.9220\n",
      "Epoch 48/50\n",
      "45324/45324 [==============================] - 3s 57us/step - loss: 0.2773 - acc: 0.9222\n",
      "Epoch 49/50\n",
      "45324/45324 [==============================] - 3s 60us/step - loss: 0.2737 - acc: 0.9240\n",
      "Epoch 50/50\n",
      "45324/45324 [==============================] - 3s 75us/step - loss: 0.2722 - acc: 0.9232\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#shortened array to make the notebook shorter\n",
    "#.005 performed the best, add more values to this array to test different regularization parameters\n",
    "lambdas = [.005]\n",
    "\n",
    "for l in lambdas:\n",
    "    \n",
    "    #Define model\n",
    "    model = Sequential([Dense(70, input_shape=(100,), W_regularizer=l2(l)), \n",
    "                       Activation('relu'),\n",
    "                       Dense(30),\n",
    "                       Activation('relu'),\n",
    "                       Dense(50),\n",
    "                       Activation('relu'),\n",
    "                       Dense(20),\n",
    "                       Activation('relu'),\n",
    "                       Dense(5),\n",
    "                       Activation('softmax'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "    #Easy way to convert to one-hot\n",
    "    y_train_hot2 = keras.utils.to_categorical(y_train, num_classes=5)\n",
    "\n",
    "    model.fit(x_train, y_train_hot2, epochs=50)\n",
    "    \n",
    "    pred_one_hot = model.predict(x_test)\n",
    "    \n",
    "    pred = []\n",
    "\n",
    "    for line in pred_one_hot:\n",
    "        pred.append(np.argmax(line))\n",
    "        \n",
    "    d = {'Id': test.index, 'y': pred}\n",
    "    out = pd.DataFrame(d)\n",
    "    out.to_csv('output_' + str(l) + '.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
